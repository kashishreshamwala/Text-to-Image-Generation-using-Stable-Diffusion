{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kashishreshamwala/Text-to-Image-Generation-using-Stable-Diffusion/blob/main/text_to_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text-to-Image Generation using Stable Diffusion\n",
        "\n",
        "\n",
        "This notebook demonstrates how to generate images from natural language prompts using the **Stable Diffusion** model from the Hugging Face `diffusers` library.  \n",
        "It also includes a simple Gradio interface to make the model interactive and user-friendly.\n"
      ],
      "metadata": {
        "id": "-dYnWNNjznN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D38e36QLpzPa",
        "outputId": "3dbca9fb-87c3-481e-b52b-5b461f45e4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct  9 11:51:01 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Libraries\n",
        "We import PyTorch for model execution, `diffusers` for the Stable Diffusion model, and Gradio for the web interface.\n"
      ],
      "metadata": {
        "id": "Nfzblc1y0Pyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay36wcUbqJcv",
        "outputId": "e16de51c-f18e-4ba8-e9b6-77c78a71a044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA version: 12.6\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'CUDA version: {torch.version.cuda}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CixM-u1Fqegi",
        "outputId": "2e85b7a0-0cb8-456b-b1d7-8acc703cea30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda device name: Tesla T4\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  print(f'cuda device name: {torch.cuda.get_device_name(0)}')\n",
        "  print(f'GPU name: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Khcz3OhMup8W"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the Stable Diffusion Model\n",
        "We load the pretrained **Stable Diffusion Pipeline** from Hugging Face.\n",
        "This model takes a text prompt as input and outputs a generated image.\n"
      ],
      "metadata": {
        "id": "_WZNJh1Z0fze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqAOp3O1wEOu"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Pytorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available}\")\n",
        "print(f'GPU device:{torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"no GPU\"}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbLA2p-CBt3D",
        "outputId": "ae307a9b-a864-4fd8-db77-6bd69306f288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version: 2.8.0+cu126\n",
            "CUDA available: <function is_available at 0x7c6f8b38c040>\n",
            "GPU device:Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Generate Images from Text Prompts\n",
        "We can now provide any text prompt, such as *\"A cat playing guitar in space\"*, and the model will generate a matching image.\n"
      ],
      "metadata": {
        "id": "_7N1-dKS0wDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "from typing import Optional, Tuple, List\n",
        "from datetime import datetime\n",
        "\n",
        "from diffusers import (\n",
        "    StableDiffusionPipeline,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DDIMScheduler,\n",
        "    LMSDiscreteScheduler\n",
        ")\n",
        "import gradio as gr\n",
        "\n",
        "# Initialization\n",
        "class StableDiffusionGenerator:\n",
        "    def __init__(self, model_id: str = \"runwayml/stable-diffusion-v1-5\", device: str = \"auto\"):\n",
        "        try:\n",
        "            self.device = self._setup_device(device)\n",
        "            self.dtype = torch.float16 if self.device.type == \"cuda\" else torch.float32\n",
        "\n",
        "            print(f\"Initializing Stable Diffusion on {self.device}\")\n",
        "            print(f\"Using precision: {self.dtype}\")\n",
        "\n",
        "            self.pipe = self._load_pipeline(model_id)\n",
        "            self.current_scheduler = \"euler_a\"\n",
        "            self.schedulers = {\n",
        "                \"euler_a\": (\"Euler Ancestral\", \"Fast, good for creative images\"),\n",
        "                \"euler\": (\"Euler\", \"Deterministic, consistent results\"),\n",
        "                \"ddim\": (\"DDIM\", \"Classic, good quality, slower\"),\n",
        "                \"dpm_solver\": (\"DPM Solver\", \"High quality, efficient\"),\n",
        "                \"lms\": (\"LMS\", \"Linear multistep, stable\")\n",
        "            }\n",
        "            print(\"Stable Diffusion Generator Ready!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Initialization Error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Setup Methods\n",
        "    def _setup_device(self, device: str) -> torch.device:\n",
        "        if device == \"auto\":\n",
        "            if torch.cuda.is_available():\n",
        "                device = \"cuda\"\n",
        "                print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
        "                vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "                print(f\"VRAM: {vram_gb:.1f}GB\")\n",
        "            else:\n",
        "                device = \"cpu\"\n",
        "                print(\"Using CPU (GPU not available)\")\n",
        "        return torch.device(device)\n",
        "\n",
        "    def _load_pipeline(self, model_id: str) -> StableDiffusionPipeline:\n",
        "        try:\n",
        "            pipe = StableDiffusionPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=self.dtype,\n",
        "                safety_checker=None,\n",
        "                requires_safety_checker=False,\n",
        "            )\n",
        "            print(\"Applying Memory Optimizations...\")\n",
        "            pipe.enable_attention_slicing()\n",
        "            pipe.enable_vae_slicing()\n",
        "\n",
        "            try:\n",
        "                pipe.enable_xformers_memory_efficient_attention()\n",
        "                print(\"XFormers Attention: Enabled\")\n",
        "            except Exception as e:\n",
        "                print(f\"XFormers: Not available ({e})\")\n",
        "\n",
        "            if self.device.type == \"cuda\":\n",
        "                try:\n",
        "                    pipe = pipe.to(self.device)\n",
        "                    print(\"Full GPU Loading: Success\")\n",
        "                except RuntimeError as e:\n",
        "                    print(\"GPU Memory Limited: Using CPU Offload\")\n",
        "                    pipe.enable_model_cpu_offload()\n",
        "            else:\n",
        "                pipe.enable_sequential_cpu_offload()\n",
        "                print(\"CPU Sequential Offload: Enabled\")\n",
        "            return pipe\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model: {e}\")\n",
        "\n",
        "# Scheduler & Generation\n",
        "    def set_scheduler(self, scheduler_name: str) -> bool:\n",
        "        if scheduler_name not in self.schedulers:\n",
        "            print(f\"Unknown scheduler: {scheduler_name}\")\n",
        "            return False\n",
        "        if scheduler_name == self.current_scheduler:\n",
        "            return True\n",
        "\n",
        "        scheduler_map = {\n",
        "            \"euler_a\": EulerAncestralDiscreteScheduler,\n",
        "            \"euler\": EulerDiscreteScheduler,\n",
        "            \"ddim\": DDIMScheduler,\n",
        "            \"dpm_solver\": DPMSolverMultistepScheduler,\n",
        "            \"lms\": LMSDiscreteScheduler\n",
        "        }\n",
        "        try:\n",
        "            scheduler_class = scheduler_map[scheduler_name]\n",
        "            self.pipe.scheduler = scheduler_class.from_config(self.pipe.scheduler.config)\n",
        "            self.current_scheduler = scheduler_name\n",
        "            name, desc = self.schedulers[scheduler_name]\n",
        "            print(f\"Scheduler Changed: {name} ({desc})\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Scheduler Error: {e}\")\n",
        "            return False\n",
        "\n",
        "# Image Generation\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str = \"\",\n",
        "        width: int = 512,\n",
        "        height: int = 512,\n",
        "        num_inference_steps: int = 20,\n",
        "        guidance_scale: float = 7.5,\n",
        "        seed: Optional[int] = None,\n",
        "        scheduler: str = \"euler_a\"\n",
        "    ) -> Tuple[Image.Image, dict]:\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"Prompt cannot be empty\")\n",
        "\n",
        "        self.set_scheduler(scheduler)\n",
        "        if seed is None:\n",
        "            seed = torch.randint(0, 2**32, (1,)).item()\n",
        "\n",
        "        generator = torch.Generator(device=self.device)\n",
        "        generator.manual_seed(seed)\n",
        "\n",
        "        width = (width // 8) * 8\n",
        "        height = (height // 8) * 8\n",
        "\n",
        "        print(f\"Generating: '{prompt[:50]}...'\")\n",
        "        print(f\"Size: {width}x{height}, Steps: {num_inference_steps}, CFG: {guidance_scale}\")\n",
        "        print(f\"Seed: {seed}, Scheduler: {scheduler}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            with torch.inference_mode():\n",
        "                if self.device.type == \"cuda\" and self.dtype == torch.float16:\n",
        "                    with autocast(self.device.type):\n",
        "                        result = self.pipe(\n",
        "                            prompt=prompt,\n",
        "                            negative_prompt=negative_prompt if negative_prompt else None,\n",
        "                            width=width,\n",
        "                            height=height,\n",
        "                            num_inference_steps=num_inference_steps,\n",
        "                            guidance_scale=guidance_scale,\n",
        "                            generator=generator\n",
        "                        )\n",
        "                else:\n",
        "                    result = self.pipe(\n",
        "                        prompt=prompt,\n",
        "                        negative_prompt=negative_prompt if negative_prompt else None,\n",
        "                        width=width,\n",
        "                        height=height,\n",
        "                        num_inference_steps=num_inference_steps,\n",
        "                        guidance_scale=guidance_scale,\n",
        "                        generator=generator\n",
        "                    )\n",
        "\n",
        "            generation_time = time.time() - start_time\n",
        "            metadata = {\n",
        "                \"prompt\": prompt,\n",
        "                \"negative_prompt\": negative_prompt,\n",
        "                \"width\": width,\n",
        "                \"height\": height,\n",
        "                \"steps\": num_inference_steps,\n",
        "                \"guidance_scale\": guidance_scale,\n",
        "                \"scheduler\": scheduler,\n",
        "                \"seed\": seed,\n",
        "                \"generation_time\": round(generation_time, 2),\n",
        "                \"device\": str(self.device),\n",
        "                \"dtype\": str(self.dtype)\n",
        "            }\n",
        "            print(f\"Generated in {generation_time:.2f}s\")\n",
        "            return result.images[0], metadata\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            self._cleanup_memory()\n",
        "            raise RuntimeError(\n",
        "                \"GPU Out of Memory! Try: reducing image size, fewer steps, \"\n",
        "                \"or use CPU mode. Current settings may be too demanding.\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Generation failed: {str(e)}\")\n",
        "        finally:\n",
        "            self._cleanup_memory()\n",
        "\n",
        "# Utility Methods\n",
        "    def _cleanup_memory(self):\n",
        "        gc.collect()\n",
        "        if self.device.type == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def get_memory_usage(self) -> dict:\n",
        "        memory_info = {}\n",
        "        if self.device.type == \"cuda\":\n",
        "            memory_info = {\n",
        "                \"allocated_gb\": torch.cuda.memory_allocated() / 1024**3,\n",
        "                \"reserved_gb\": torch.cuda.memory_reserved() / 1024**3,\n",
        "                \"max_allocated_gb\": torch.cuda.max_memory_allocated() / 1024**3,\n",
        "                \"total_gb\": torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            }\n",
        "        else:\n",
        "            memory_info = {\"device\": \"cpu\", \"note\": \"CPU memory tracking not available\"}\n",
        "        return memory_info\n",
        "\n",
        "    def save_image(self, image: Image.Image, metadata: dict, output_dir: str = \"outputs\") -> str:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"sd_gen_{timestamp}_s{metadata['seed']}_{metadata['width']}x{metadata['height']}.png\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        image.save(filepath)\n",
        "\n",
        "        metadata_file = filepath.replace('.png', '_metadata.txt')\n",
        "        with open(metadata_file, 'w') as f:\n",
        "            f.write(\"Stable Diffusion Generation Metadata\\n\")\n",
        "            f.write(\"=\" * 40 + \"\\n\")\n",
        "            for key, value in metadata.items():\n",
        "                f.write(f\"{key}: {value}\\n\")\n",
        "        print(f\"Saved: {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "# Initialization & Generator Setup\n",
        "class StableDiffusionUI:\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.gallery_images = []\n",
        "        self.generation_history = []\n",
        "\n",
        "    def initialize_generator(self, model_choice: str, device_choice: str) -> str:\n",
        "        try:\n",
        "            model_map = {\n",
        "                \"Stable Diffusion 1.5 (Recommended)\": \"runwayml/stable-diffusion-v1-5\",\n",
        "                \"Stable Diffusion 2.1\": \"stabilityai/stable-diffusion-2-1\",\n",
        "                \"Realistic Vision (RealVisXL)\": \"SG161222/RealVisXL_V4.0\"\n",
        "            }\n",
        "            device_map = {\n",
        "                \"Auto (Recommended)\": \"auto\",\n",
        "                \"GPU (CUDA)\": \"cuda\",\n",
        "                \"CPU (Slower)\": \"cpu\"\n",
        "            }\n",
        "            model_id = model_map.get(model_choice, \"runwayml/stable-diffusion-v1-5\")\n",
        "            device = device_map.get(device_choice, \"auto\")\n",
        "\n",
        "            self.generator = StableDiffusionGenerator(model_id=model_id, device=device)\n",
        "            memory_info = self.generator.get_memory_usage()\n",
        "            memory_text = f\"Memory Usage: {memory_info}\" if memory_info else \"Ready!\"\n",
        "            return f\"Model loaded successfully!\\n{memory_text}\"\n",
        "        except Exception as e:\n",
        "            return f\"Initialization failed: {str(e)}\"\n",
        "\n",
        "# Image Generation Handler\n",
        "    def generate_image(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        width: int,\n",
        "        height: int,\n",
        "        steps: int,\n",
        "        guidance: float,\n",
        "        scheduler: str,\n",
        "        seed: int,\n",
        "        save_image: bool\n",
        "    ) -> Tuple[Optional[Image.Image], str, str]:\n",
        "        if self.generator is None:\n",
        "            return None, \"Please initialize the model first!\", \"\"\n",
        "        if not prompt.strip():\n",
        "            return None, \"Please enter a prompt!\", \"\"\n",
        "\n",
        "        try:\n",
        "            seed = None if seed == -1 else int(seed)\n",
        "            image, metadata = self.generator.generate_image(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                width=width,\n",
        "                height=height,\n",
        "                num_inference_steps=steps,\n",
        "                guidance_scale=guidance,\n",
        "                scheduler=scheduler,\n",
        "                seed=seed\n",
        "            )\n",
        "\n",
        "            info_text = self._format_generation_info(metadata)\n",
        "            saved_path = \"\"\n",
        "            if save_image:\n",
        "                saved_path = self.generator.save_image(image, metadata)\n",
        "\n",
        "            self.generation_history.append(metadata)\n",
        "            self.gallery_images.append(image)\n",
        "\n",
        "            if len(self.gallery_images) > 10:\n",
        "                self.gallery_images = self.gallery_images[-10:]\n",
        "                self.generation_history = self.generation_history[-10:]\n",
        "\n",
        "            return image, info_text, saved_path\n",
        "        except Exception as e:\n",
        "            return None, f\"Generation failed: {str(e)}\", \"\"\n",
        "\n",
        "# CELL 10: UI Class - Part 3 (Helper Methods)\n",
        "    def _format_generation_info(self, metadata: dict) -> str:\n",
        "        return f\"\"\"\n",
        "Generation Complete!\n",
        "\n",
        "Parameters Used:\n",
        "- Prompt: {metadata['prompt'][:100]}{'...' if len(metadata['prompt']) > 100 else ''}\n",
        "- Size: {metadata['width']} x {metadata['height']} pixels\n",
        "- Steps: {metadata['steps']} (more steps = higher quality, slower)\n",
        "- Guidance Scale: {metadata['guidance_scale']} (higher = follows prompt more closely)\n",
        "- Scheduler: {metadata['scheduler']}\n",
        "- Seed: {metadata['seed']} (for reproducible results)\n",
        "\n",
        "Performance:\n",
        "- Generation Time: {metadata['generation_time']}s\n",
        "- Device: {metadata['device']}\n",
        "- Precision: {metadata['dtype']}\n",
        "\"\"\"\n",
        "\n",
        "    def get_example_prompts(self) -> list:\n",
        "        return [\n",
        "            [\"a serene mountain landscape at sunrise, photorealistic, highly detailed\", \"blurry, low quality\"],\n",
        "            [\"portrait of a wise old wizard, fantasy art, digital painting\", \"ugly, deformed\"],\n",
        "            [\"cyberpunk cityscape at night, neon lights, futuristic\", \"daytime, bright\"],\n",
        "            [\"cute cartoon cat wearing a hat, kawaii style\", \"realistic, scary\"],\n",
        "            [\"abstract geometric patterns, colorful, modern art\", \"representational, dull colors\"]\n",
        "        ]\n",
        "\n",
        "    def show_scheduler_info(self, scheduler: str) -> str:\n",
        "        scheduler_info = {\n",
        "            \"euler_a\": \"Euler Ancestral: Fast and creative, adds slight randomness for variety\",\n",
        "            \"euler\": \"Euler: Deterministic and consistent, same seed = same result\",\n",
        "            \"ddim\": \"DDIM: Classic scheduler, high quality but slower\",\n",
        "            \"dpm_solver\": \"DPM Solver: Efficient high-quality generation\",\n",
        "            \"lms\": \"LMS: Linear multistep, very stable results\"\n",
        "        }\n",
        "        return scheduler_info.get(scheduler, \"Scheduler information not available\")\n",
        "\n",
        "    def get_memory_info(self) -> str:\n",
        "        if self.generator is None:\n",
        "            return \"Model not loaded\"\n",
        "        try:\n",
        "            memory_info = self.generator.get_memory_usage()\n",
        "            if 'allocated_gb' in memory_info:\n",
        "                return f\"\"\"\n",
        "GPU Memory Usage:\n",
        "- Allocated: {memory_info['allocated_gb']:.2f}GB\n",
        "- Reserved: {memory_info['reserved_gb']:.2f}GB\n",
        "- Total Available: {memory_info['total_gb']:.2f}GB\n",
        "- Usage: {(memory_info['allocated_gb']/memory_info['total_gb']*100):.1f}%\n",
        "                \"\"\"\n",
        "            else:\n",
        "                return \"CPU mode - memory tracking not available\"\n",
        "        except:\n",
        "            return \"Memory info unavailable\"\n",
        "\n",
        "# Interface Creation\n",
        "    def create_interface(self) -> gr.Blocks:\n",
        "        with gr.Blocks(\n",
        "            title=\"Educational Stable Diffusion Generator\",\n",
        "            theme=gr.themes.Soft()\n",
        "        ) as interface:\n",
        "            gr.Markdown(\"\"\"\n",
        "            # Educational Stable Diffusion Text-to-Image Generator\n",
        "            **Learn Generative AI concepts while creating images!**\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Tab(\"Setup & Generation\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### Model Setup\")\n",
        "                        model_choice = gr.Dropdown(\n",
        "                            choices=[\n",
        "                                \"Stable Diffusion 1.5 (Recommended)\",\n",
        "                                \"Stable Diffusion 2.1\",\n",
        "                                \"Realistic Vision (RealVisXL)\"\n",
        "                            ],\n",
        "                            value=\"Stable Diffusion 1.5 (Recommended)\",\n",
        "                            label=\"Model Selection\"\n",
        "                        )\n",
        "                        device_choice = gr.Dropdown(\n",
        "                            choices=[\n",
        "                                \"Auto (Recommended)\",\n",
        "                                \"GPU (CUDA)\",\n",
        "                                \"CPU (Slower)\"\n",
        "                            ],\n",
        "                            value=\"Auto (Recommended)\",\n",
        "                            label=\"Device Selection\"\n",
        "                        )\n",
        "                        init_btn = gr.Button(\"Initialize Model\", variant=\"primary\")\n",
        "                        init_status = gr.Textbox(\n",
        "                            label=\"Initialization Status\",\n",
        "                            placeholder=\"Click Initialize Model to start\",\n",
        "                            lines=3\n",
        "                        )\n",
        "                    with gr.Column():\n",
        "                        gr.Markdown(\"### System Info\")\n",
        "                        memory_btn = gr.Button(\"Check Memory Usage\")\n",
        "                        memory_info = gr.Textbox(\n",
        "                            label=\"Memory Information\",\n",
        "                            placeholder=\"Click to check memory usage\",\n",
        "                            lines=6\n",
        "                        )\n",
        "\n",
        "                gr.Markdown(\"### Image Generation\")\n",
        "                with gr.Row():\n",
        "                    with gr.Column():\n",
        "                        prompt = gr.Textbox(\n",
        "                            label=\"Prompt (Describe what you want)\",\n",
        "                            placeholder=\"a beautiful landscape painting, oil on canvas, detailed\",\n",
        "                            lines=3\n",
        "                        )\n",
        "                        negative_prompt = gr.Textbox(\n",
        "                            label=\"Negative Prompt (What to avoid)\",\n",
        "                            placeholder=\"blurry, low quality, bad anatomy\",\n",
        "                            lines=2\n",
        "                        )\n",
        "                        generate_btn = gr.Button(\"Generate Image\", variant=\"primary\", size=\"lg\")\n",
        "                    with gr.Column():\n",
        "                        with gr.Accordion(\"Advanced Settings\", open=True):\n",
        "                            with gr.Row():\n",
        "                                width = gr.Slider(256, 1024, 512, step=64, label=\"Width\")\n",
        "                                height = gr.Slider(256, 1024, 512, step=64, label=\"Height\")\n",
        "                            with gr.Row():\n",
        "                                steps = gr.Slider(10, 100, 20, step=1, label=\"Inference Steps\")\n",
        "                                guidance = gr.Slider(1.0, 20.0, 7.5, step=0.5, label=\"Guidance Scale\")\n",
        "                            scheduler = gr.Dropdown(\n",
        "                                choices=[\"euler_a\", \"euler\", \"ddim\", \"dpm_solver\", \"lms\"],\n",
        "                                value=\"euler_a\",\n",
        "                                label=\"Scheduler\"\n",
        "                            )\n",
        "                            scheduler_info = gr.Textbox(\n",
        "                                label=\"Scheduler Information\",\n",
        "                                interactive=False,\n",
        "                                lines=2\n",
        "                            )\n",
        "                            with gr.Row():\n",
        "                                seed = gr.Number(-1, label=\"Seed\")\n",
        "                                save_image = gr.Checkbox(True, label=\"Save Generated Images\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    output_image = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
        "                with gr.Row():\n",
        "                    generation_info = gr.Textbox(\n",
        "                        label=\"Generation Information\",\n",
        "                        lines=10,\n",
        "                        interactive=False\n",
        "                    )\n",
        "                    saved_path = gr.Textbox(\n",
        "                        label=\"Saved File Path\",\n",
        "                        interactive=False\n",
        "                    )\n",
        "\n",
        "            with gr.Tab(\"Learning Resources\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                ## Understanding Stable Diffusion\n",
        "                ### What is Diffusion?\n",
        "                Diffusion models learn to gradually remove noise from random data.\n",
        "                ### Key Components:\n",
        "                **CLIP (Text Encoder)**\n",
        "                **U-Net (Denoising Network)**\n",
        "                **VAE (Variational Autoencoder)**\n",
        "                **Schedulers**\n",
        "                ### Parameter Guide:\n",
        "                **Steps (10-100)**: More steps = higher quality but slower generation\n",
        "                **Guidance Scale (1-20)**: Higher values make the AI follow your prompt more strictly\n",
        "                **Seed**: Controls randomness - same seed + settings = same image\n",
        "                **Resolution**: Higher resolution = more detail but needs more GPU memory\n",
        "                \"\"\")\n",
        "\n",
        "            with gr.Tab(\"Examples & Gallery\"):\n",
        "                gr.Markdown(\"### Example Prompts to Try\")\n",
        "                examples = gr.Examples(\n",
        "                    examples=self.get_example_prompts(),\n",
        "                    inputs=[prompt, negative_prompt],\n",
        "                    label=\"Click any example to load it\"\n",
        "                )\n",
        "                gr.Markdown(\"### Recent Generations\")\n",
        "                gallery = gr.Gallery(\n",
        "                    value=[],\n",
        "                    label=\"Your Generated Images\",\n",
        "                    show_label=True,\n",
        "                    elem_id=\"gallery\",\n",
        "                    columns=3,\n",
        "                    rows=2,\n",
        "                    object_fit=\"contain\",\n",
        "                    height=\"auto\"\n",
        "                )\n",
        "\n",
        "            # Event handlers\n",
        "            init_btn.click(\n",
        "                fn=self.initialize_generator,\n",
        "                inputs=[model_choice, device_choice],\n",
        "                outputs=init_status\n",
        "            )\n",
        "            generate_btn.click(\n",
        "                fn=self.generate_image,\n",
        "                inputs=[prompt, negative_prompt, width, height, steps, guidance, scheduler, seed, save_image],\n",
        "                outputs=[output_image, generation_info, saved_path]\n",
        "            ).then(\n",
        "                fn=lambda: self.gallery_images,\n",
        "                outputs=gallery\n",
        "            )\n",
        "            scheduler.change(\n",
        "                fn=self.show_scheduler_info,\n",
        "                inputs=scheduler,\n",
        "                outputs=scheduler_info\n",
        "            )\n",
        "            memory_btn.click(\n",
        "                fn=self.get_memory_info,\n",
        "                outputs=memory_info\n",
        "            )\n",
        "\n",
        "        return interface\n",
        "\n",
        "# Launch the Application\n",
        "ui = StableDiffusionUI()\n",
        "interface = ui.create_interface()\n",
        "interface.launch(\n",
        "    share=True,\n",
        "    server_name=\"0.0.0.0\",\n",
        "    server_port=7860,\n",
        "    debug=True,\n",
        "    show_error=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "IKDMtEoxCgnc",
        "outputId": "4b0308f1-89a6-4477-ca7d-50f6d537cb2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://31f38278ba9e52b30e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://31f38278ba9e52b30e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mWquYyPr3hX5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPDcZHZtqP+ybqaCzejSK8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}